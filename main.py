#!/usr/bin/env python3
"""
üéØ Attention Optimization AI - HEATMAP FIXED VERSION
Professional CRO & UX Analysis powered by OpenAI Vision
"""

import os
import io
import json
import base64
import uuid
import time
import logging
from typing import List, Tuple, Dict, Any, Optional
from pathlib import Path

# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# Core dependencies
from fastapi import FastAPI, Request, UploadFile, Form, HTTPException, File
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from PIL import Image, ImageDraw, ImageFilter, ImageEnhance
import requests
import numpy as np
from scipy.ndimage import gaussian_filter

# Optional dependencies
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# ============================================================================
# CONFIGURATION
# ============================================================================

class Config:
    DEBUG = os.getenv("DEBUG", "false").lower() == "true"
    HOST = os.getenv("HOST", "0.0.0.0")
    PORT = int(os.getenv("PORT", "8080"))
    
    # OpenAI
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
    TEMPERATURE = float(os.getenv("TEMPERATURE", "0.2"))
    MAX_IMAGE_SIZE = int(os.getenv("MAX_IMAGE_SIZE", "2048"))
    
    # Paths
    BASE_DIR = Path(__file__).parent
    OUTPUTS_DIR = BASE_DIR / "outputs"
    STATIC_DIR = BASE_DIR / "static"
    TEMPLATES_DIR = BASE_DIR / "templates"

# Ensure directories exist
for dir_path in [Config.OUTPUTS_DIR, Config.STATIC_DIR, Config.TEMPLATES_DIR]:
    dir_path.mkdir(exist_ok=True)

# ============================================================================
# LOGGING SETUP
# ============================================================================

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger("attention-ai")

# ============================================================================
# SIMPLIFIED AI ANALYSIS SCHEMA - FIXED FOR HEATMAPS
# ============================================================================

ANALYSIS_SCHEMA = {
    "name": "attention_analysis",
    "strict": True,
    "schema": {
        "type": "object",
        "additionalProperties": False,
        "properties": {
            "saliency_grid": {
                "type": "array",
                "items": {
                    "type": "array", 
                    "items": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                }
            },
            "ctas": {
                "type": "array",
                "items": {
                    "type": "object",
                    "additionalProperties": False,
                    "properties": {
                        "text": {"type": "string"},
                        "bbox": {"type": "array", "items": {"type": "number"}, "minItems": 4, "maxItems": 4},
                        "saliency": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                        "priority": {"type": "integer", "minimum": 1, "maximum": 5},
                        "issues": {"type": "array", "items": {"type": "string"}},
                        "improvement": {"type": "string"},
                        "contrast": {"type": "number", "minimum": 0.0, "maximum": 5.0}
                    },
                    "required": ["text", "bbox", "saliency", "priority", "issues", "improvement", "contrast"]
                }
            },
            "quick_wins": {
                "type": "array",
                "items": {"type": "string"}
            },
            "major_improvements": {
                "type": "array",
                "items": {"type": "string"}
            },
            "ab_tests": {
                "type": "array",
                "items": {
                    "type": "object",
                    "additionalProperties": False,
                    "properties": {
                        "title": {"type": "string"},
                        "hypothesis": {"type": "string"},
                        "variant_a": {"type": "string"},
                        "variant_b": {"type": "string"},
                        "expected_lift": {"type": "string"},
                        "difficulty": {"type": "string"}
                    },
                    "required": ["title", "hypothesis", "variant_a", "variant_b", "expected_lift", "difficulty"]
                }
            },
            "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}
        },
        "required": ["saliency_grid", "ctas", "quick_wins", "major_improvements", "ab_tests", "confidence"]
    }
}

SYSTEM_PROMPT = """You are a world-class CRO expert analyzing landing pages for attention optimization.

CRITICAL: You MUST provide a complete 12x16 saliency grid (12 rows, 16 columns) with realistic attention values.

For SALIENCY GRID:
1. Create EXACTLY a 12x16 grid (12 rows, 16 columns) 
2. Values 0.0-1.0 representing attention probability
3. Use varied values - don't make everything the same!
4. Higher values (0.7-1.0) for: CTAs, headlines, hero images, faces
5. Medium values (0.4-0.6) for: text content, secondary images
6. Lower values (0.1-0.3) for: backgrounds, whitespace, footer
7. Make the grid realistic - create clear hotspots and cool areas

For CTAs:
- Detect ALL call-to-action elements (buttons, forms, links)
- Provide bounding box [x1, y1, x2, y2] normalized 0-1
- Calculate saliency score based on visual prominence
- Assign priority 1-5 (1=primary, 5=least important)
- Identify specific issues and improvements

EXAMPLE of good saliency grid structure:
- Top area (hero section): values 0.6-0.9
- CTA areas: values 0.8-1.0  
- Text areas: values 0.3-0.6
- Background/whitespace: values 0.1-0.3
- Footer: values 0.1-0.2

Focus on creating a realistic attention heatmap that shows clear patterns."""

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def image_to_base64(image: Image.Image) -> str:
    """Convert PIL image to base64 data URL"""
    buffer = io.BytesIO()
    image.save(buffer, format="PNG")
    b64_string = base64.b64encode(buffer.getvalue()).decode()
    return f"data:image/png;base64,{b64_string}"

def downscale_image(image: Image.Image, max_size: int = 2048) -> Image.Image:
    """Downscale image if too large"""
    width, height = image.size
    if max(width, height) <= max_size:
        return image
    
    if width >= height:
        new_width = max_size
        new_height = int(height * (max_size / width))
    else:
        new_height = max_size
        new_width = int(width * (max_size / height))
    
    return image.resize((new_width, new_height), Image.LANCZOS)

# ============================================================================
# FIXED OPENAI INTEGRATION
# ============================================================================

async def analyze_with_openai(image: Image.Image, html: str = "") -> Dict[str, Any]:
    """COMPLETELY FIXED OpenAI Vision API integration"""
    if not Config.OPENAI_API_KEY or not OPENAI_AVAILABLE:
        raise HTTPException(500, "OpenAI not configured. Set OPENAI_API_KEY environment variable.")
    
    try:
        client = OpenAI(api_key=Config.OPENAI_API_KEY)
        logger.info("‚úÖ OpenAI client initialized successfully")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize OpenAI client: {e}")
        raise HTTPException(500, f"OpenAI client initialization failed: {str(e)}")
    
    # Prepare image
    processed_image = downscale_image(image, Config.MAX_IMAGE_SIZE)
    image_data_url = image_to_base64(processed_image)
    
    # Create enhanced prompt with specific grid requirements
    user_prompt = f"""Analyze this landing page for attention optimization.

CRITICAL REQUIREMENTS:
1. Create a complete 12x16 saliency grid (12 rows, 16 columns)
2. Use varied attention values (0.0 to 1.0) to create clear hotspots
3. Higher values for prominent elements like CTAs and headlines
4. Lower values for backgrounds and whitespace
5. Detect all visible CTAs with accurate bounding boxes

Focus on:
- Visual hierarchy and attention flow
- CTA visibility and effectiveness  
- Color psychology and contrast
- Typography and readability
- Layout patterns

HTML context: {html[:2000]}

Return a complete analysis with a realistic attention heatmap."""
    
    logger.info(f"ü§ñ Calling OpenAI API with model: {Config.OPENAI_MODEL}")
    
    try:
        response = client.chat.completions.create(
            model=Config.OPENAI_MODEL,
            temperature=Config.TEMPERATURE,
            max_tokens=4000,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": [
                    {"type": "text", "text": user_prompt},
                    {"type": "image_url", "image_url": {"url": image_data_url}}
                ]}
            ],
            response_format={"type": "json_schema", "json_schema": ANALYSIS_SCHEMA}
        )
        
        content = response.choices[0].message.content
        result = json.loads(content)
        
        # Validate and fix saliency grid if needed
        saliency_grid = result.get('saliency_grid', [])
        if not saliency_grid or len(saliency_grid) == 0:
            logger.warning("‚ùå Empty saliency grid received, creating fallback")
            result['saliency_grid'] = create_fallback_saliency_grid()
        else:
            logger.info(f"‚úÖ Saliency grid received: {len(saliency_grid)}x{len(saliency_grid[0]) if saliency_grid else 0}")
        
        logger.info(f"‚ú® OpenAI analysis completed successfully")
        return result
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå JSON parsing error: {e}")
        raise HTTPException(500, f"AI response parsing failed: {str(e)}")
    except Exception as e:
        logger.error(f"‚ùå OpenAI API error: {e}")
        raise HTTPException(500, f"AI analysis failed: {str(e)}")

def create_fallback_saliency_grid() -> List[List[float]]:
    """Create a fallback saliency grid if AI doesn't provide one"""
    grid = []
    for i in range(12):  # 12 rows
        row = []
        for j in range(16):  # 16 columns
            # Create a simple attention pattern
            # Higher attention in center-top area, lower at edges
            center_distance = abs(j - 8) / 8.0  # Distance from horizontal center
            vertical_weight = max(0, 1 - i / 12.0)  # Higher at top
            
            base_attention = 0.2 + (1 - center_distance) * vertical_weight * 0.6
            
            # Add some randomness for realism
            import random
            noise = random.uniform(-0.1, 0.1)
            attention = max(0.0, min(1.0, base_attention + noise))
            
            row.append(attention)
        grid.append(row)
    
    logger.info("üìä Created fallback 12x16 saliency grid")
    return grid

# ============================================================================
# ENHANCED IMAGE PROCESSING & VISUALIZATION - FIXED HEATMAPS
# ============================================================================

def create_vibrant_heatmap(saliency_grid: List[List[float]], image_size: Tuple[int, int]) -> Image.Image:
    """Create a VIBRANT, clearly visible heatmap"""
    rows, cols = len(saliency_grid), len(saliency_grid[0]) if saliency_grid else 0
    
    logger.info(f"üé® Creating heatmap: {rows}x{cols} grid for {image_size} image")
    
    if rows == 0 or cols == 0:
        logger.warning("‚ùå Empty saliency grid, creating transparent overlay")
        return Image.new("RGBA", image_size, (0, 0, 0, 0))
    
    # Convert to numpy array
    arr = np.array(saliency_grid, dtype=float)
    logger.info(f"üìä Saliency range: {arr.min():.3f} to {arr.max():.3f}")
    
    # Normalize to ensure we use the full range
    if arr.max() > arr.min():
        arr = (arr - arr.min()) / (arr.max() - arr.min())
    
    # Apply strong Gaussian smoothing for better visual appeal
    smoothed = gaussian_filter(arr, sigma=1.2)
    
    # Create VIBRANT colormap with high visibility
    def apply_vibrant_colormap(value):
        """Apply a vibrant, highly visible colormap"""
        # Increase minimum alpha for better visibility
        base_alpha = 80  # Minimum visibility
        max_alpha = 180  # Maximum visibility
        
        alpha = int(base_alpha + (max_alpha - base_alpha) * value)
        
        if value < 0.25:
            # Cool blue for low attention
            return (30, 144, 255, alpha)  # Dodger blue
        elif value < 0.5:
            # Green transition
            return (50, 205, 50, alpha)  # Lime green
        elif value < 0.75:
            # Orange for medium-high attention
            return (255, 165, 0, alpha)  # Orange
        else:
            # Hot red for high attention
            return (255, 69, 0, alpha)  # Red orange
    
    # Apply colormap
    colored_array = np.zeros((rows, cols, 4), dtype=np.uint8)
    for i in range(rows):
        for j in range(cols):
            r, g, b, a = apply_vibrant_colormap(smoothed[i, j])
            colored_array[i, j] = [r, g, b, a]
    
    # Create image and resize with high-quality resampling
    heatmap_small = Image.fromarray(colored_array, 'RGBA')
    heatmap = heatmap_small.resize(image_size, Image.LANCZOS)
    
    # Apply subtle blur for smoothness
    blurred = heatmap.filter(ImageFilter.GaussianBlur(radius=2))
    
    logger.info("‚ú® Vibrant heatmap created successfully")
    return blurred

def draw_enhanced_cta_boxes(image: Image.Image, ctas: List[Dict]) -> Image.Image:
    """Draw enhanced CTA bounding boxes with better styling"""
    if not ctas:
        logger.info("üì¶ No CTAs to draw")
        return image
    
    logger.info(f"üì¶ Drawing {len(ctas)} CTA boxes")
    
    img_with_boxes = image.convert('RGBA')
    overlay = Image.new('RGBA', image.size, (0, 0, 0, 0))
    draw = ImageDraw.Draw(overlay)
    
    width, height = image.size
    
    # Priority color scheme - MORE VIBRANT
    priority_colors = {
        1: (255, 20, 60),   # Crimson - Critical
        2: (255, 140, 0),   # Dark Orange - High  
        3: (0, 255, 127),   # Spring Green - Medium
        4: (0, 191, 255),   # Deep Sky Blue - Low
        5: (169, 169, 169)  # Dark Gray - Minimal
    }
    
    for i, cta in enumerate(ctas):
        bbox = cta.get('bbox', [0, 0, 0.1, 0.1])
        
        # Convert to pixel coordinates
        x1, y1, x2, y2 = [
            int(bbox[0] * width),
            int(bbox[1] * height), 
            int(bbox[2] * width),
            int(bbox[3] * height)
        ]
        
        priority = cta.get('priority', 3)
        base_color = priority_colors.get(priority, priority_colors[3])
        
        logger.info(f"   CTA {i+1}: '{cta.get('text', 'Unknown')}' at ({x1},{y1},{x2},{y2}) priority {priority}")
        
        # Draw enhanced box with THICK, VISIBLE borders
        # Outer glow effect
        for thickness in range(8, 0, -1):
            alpha = int(40 + (8 - thickness) * 15)
            glow_color = base_color + (alpha,)
            draw.rectangle([x1-thickness, y1-thickness, x2+thickness, y2+thickness], 
                         outline=glow_color, width=2)
        
        # Main border - THICK and VISIBLE
        draw.rectangle([x1, y1, x2, y2], outline=base_color + (255,), width=5)
        
        # Corner indicators - LARGER
        corner_size = 12
        corners = [(x1, y1), (x2-corner_size, y1), (x1, y2-corner_size), (x2-corner_size, y2-corner_size)]
        for corner in corners:
            draw.rectangle([corner[0], corner[1], corner[0]+corner_size, corner[1]+corner_size],
                         fill=base_color + (220,))
        
        # Enhanced label
        text = cta.get('text', f'CTA {i+1}')[:30] + ('...' if len(cta.get('text', '')) > 30 else '')
        saliency = cta.get('saliency', 0)
        label = f"P{priority}: {text} ({saliency:.1%})"
        
        # Calculate label dimensions
        label_width = max(len(label) * 9 + 20, 200)
        label_height = 35
        
        # Position label to avoid overlaps
        label_y = y1 - label_height - 8 if y1 > label_height + 15 else y2 + 8
        label_x = max(0, min(x1, width - label_width))
        
        # Draw label background with gradient effect
        draw.rectangle([label_x, label_y, label_x + label_width, label_y + label_height],
                      fill=base_color + (240,))
        
        # Draw label border
        draw.rectangle([label_x, label_y, label_x + label_width, label_y + label_height],
                      outline=(255, 255, 255, 255), width=2)
        
        # Priority circle
        circle_x = label_x + 12
        circle_y = label_y + 12
        draw.ellipse([circle_x, circle_y, circle_x + 16, circle_y + 16],
                    fill=(255, 255, 255, 255))
        draw.ellipse([circle_x+1, circle_y+1, circle_x + 15, circle_y + 15],
                    outline=base_color + (255,), width=2)
        
        # Priority text
        draw.text((circle_x + 5, circle_y + 2), str(priority), 
                 fill=(0, 0, 0, 255))
        
        # Label text - WHITE for high contrast
        draw.text((label_x + 35, label_y + 8), f"{text} ({saliency:.0%})", 
                 fill=(255, 255, 255, 255))
    
    # Composite the overlay
    result = Image.alpha_composite(img_with_boxes, overlay)
    logger.info("‚úÖ CTA boxes drawn successfully")
    return result

def create_annotated_image(image: Image.Image, analysis: Dict[str, Any]) -> Tuple[Image.Image, Dict]:
    """Create annotated image with VIBRANT heatmap and CTA boxes"""
    
    logger.info("üé® Creating annotated image...")
    
    # Create VIBRANT heatmap
    saliency_grid = analysis.get('saliency_grid', [])
    if not saliency_grid:
        logger.warning("‚ùå No saliency grid found in analysis")
        saliency_grid = create_fallback_saliency_grid()
    
    heatmap = create_vibrant_heatmap(saliency_grid, image.size)
    
    # Composite heatmap with image
    base_image = image.convert('RGBA')
    annotated_image = Image.alpha_composite(base_image, heatmap)
    
    logger.info("‚úÖ Heatmap applied to image")
    
    # Add CTA boxes
    ctas = analysis.get('ctas', [])
    logger.info(f"üì¶ Found {len(ctas)} CTAs to process")
    
    if ctas:
        annotated_image = draw_enhanced_cta_boxes(annotated_image, ctas)
    else:
        logger.warning("‚ùå No CTAs found in analysis")
    
    # Calculate metrics
    metrics = calculate_metrics(analysis, image.size)
    
    logger.info("‚úÖ Annotated image created successfully")
    return annotated_image, metrics

def calculate_metrics(analysis: Dict[str, Any], image_size: Tuple[int, int]) -> Dict:
    """Calculate performance metrics"""
    ctas = analysis.get('ctas', [])
    saliency_grid = analysis.get('saliency_grid', [])
    
    # Basic metrics
    num_ctas = len(ctas)
    avg_saliency = np.mean([cta.get('saliency', 0) for cta in ctas]) if ctas else 0.0
    total_contrast = sum(cta.get('contrast', 0) for cta in ctas)
    confidence = analysis.get('confidence', 0.0)
    
    # Overall score calculation
    overall_score = 0
    primary_cta = None
    if ctas:
        primary_cta = max(ctas, key=lambda x: x.get('saliency', 0))
        primary_saliency = primary_cta.get('saliency', 0)
        
        # Score based on primary CTA performance
        overall_score = min(100, primary_saliency * 100 + (avg_saliency * 30) + (confidence * 20))
    
    # Flow score based on saliency distribution
    flow_score = 0
    if saliency_grid:
        grid_array = np.array(saliency_grid)
        if grid_array.size > 0:
            # Good flow has clear peaks and valleys
            variance = np.var(grid_array)
            flow_score = min(1.0, variance * 2)
    
    return {
        'num_ctas': num_ctas,
        'avg_saliency': avg_saliency,
        'total_contrast': total_contrast if total_contrast > 0 else 2.5,  # Fallback
        'confidence': confidence,
        'overall_score': overall_score,
        'flow_score': flow_score,
        'primary_cta': primary_cta
    }

# ============================================================================
# WEB SCRAPING & SCREENSHOTS
# ============================================================================

async def screenshot_url(url: str, full_page: bool = True) -> Optional[Image.Image]:
    """Take screenshot of URL using Playwright"""
    if not PLAYWRIGHT_AVAILABLE:
        logger.warning("‚ö†Ô∏è Playwright not available for screenshots")
        return None
    
    try:
        logger.info(f"üì∏ Starting screenshot for: {url}")
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page(viewport={'width': 1440, 'height': 900})
            
            await page.goto(url, wait_until='networkidle', timeout=30000)
            
            if full_page:
                height = await page.evaluate('document.body.scrollHeight')
                await page.set_viewport_size({'width': 1440, 'height': height})
            
            screenshot_bytes = await page.screenshot(full_page=full_page)
            await browser.close()
            
            image = Image.open(io.BytesIO(screenshot_bytes)).convert('RGB')
            logger.info(f"‚úÖ Screenshot successful: {image.size}")
            return image
            
    except Exception as e:
        logger.error(f"‚ùå Screenshot failed for {url}: {e}")
        return None

def fetch_page_html(url: str) -> str:
    """Fetch HTML content from URL"""
    try:
        logger.info(f"üåê Fetching HTML from: {url}")
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        html = response.text[:15000]
        logger.info(f"‚úÖ HTML fetched: {len(html)} characters")
        return html
    except Exception as e:
        logger.error(f"‚ùå Failed to fetch HTML for {url}: {e}")
        return ""

# ============================================================================
# FASTAPI APPLICATION - COMPLETELY FIXED
# ============================================================================

app = FastAPI(title="üéØ Attention Optimization AI", version="2.0.0-heatmap-fixed")

# Mount static files
app.mount("/static", StaticFiles(directory=Config.STATIC_DIR), name="static")
app.mount("/outputs", StaticFiles(directory=Config.OUTPUTS_DIR), name="outputs")

# Templates
templates = Jinja2Templates(directory=Config.TEMPLATES_DIR)

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log requests with timing"""
    start_time = time.time()
    request_id = str(uuid.uuid4())[:8]
    
    logger.info(f"üöÄ Request {request_id}: {request.method} {request.url.path}")
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(f"‚úÖ Request {request_id} completed in {process_time:.2f}s")
    
    return response

@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    """Enhanced main page"""
    status = {
        'openai_available': OPENAI_AVAILABLE and bool(Config.OPENAI_API_KEY),
        'playwright_available': PLAYWRIGHT_AVAILABLE,
        'model': Config.OPENAI_MODEL
    }
    
    return templates.TemplateResponse("index.html", {
        "request": request,
        "status": status
    })

@app.post("/analyze", response_class=HTMLResponse)
async def analyze(
    request: Request,
    url: Optional[str] = Form(None),
    file: Optional[UploadFile] = File(None)
):
    """COMPLETELY FIXED analysis endpoint with VIBRANT HEATMAPS"""
    request_id = str(uuid.uuid4())[:8]
    
    logger.info(f"üîç Analysis request {request_id}")
    logger.info(f"   URL: '{url}' (length: {len(url) if url else 0})")
    logger.info(f"   File: {file.filename if file and hasattr(file, 'filename') else 'None'}")
    
    try:
        # Get image and HTML
        image = None
        html = ""
        
        # File processing
        if file and hasattr(file, 'filename') and file.filename and file.filename.strip():
            try:
                logger.info(f"üìÅ Processing uploaded file: {file.filename}")
                content = await file.read()
                
                if len(content) == 0:
                    return HTMLResponse(
                        '<div class="error-message">‚ö†Ô∏è Uploaded file is empty. Please try again.</div>',
                        status_code=400
                    )
                
                image = Image.open(io.BytesIO(content)).convert('RGB')
                logger.info(f"‚úÖ Image processed: {image.size}")
                
            except Exception as e:
                logger.error(f"‚ùå File processing error: {e}")
                return HTMLResponse(
                    '<div class="error-message">‚ùå Invalid image file. Please upload a valid PNG, JPEG, or WebP image.</div>',
                    status_code=400
                )
            
        # URL processing
        elif url and url.strip():
            clean_url = url.strip()
            if not clean_url.startswith(('http://', 'https://')):
                clean_url = f'https://{clean_url}'
            
            logger.info(f"üåê Processing URL: {clean_url}")
            
            image = await screenshot_url(clean_url, True)
            if not image:
                return HTMLResponse(
                    '<div class="error-message">üì∏ Failed to capture screenshot. Please check the URL and try again.</div>',
                    status_code=400
                )
            
            html = fetch_page_html(clean_url)
            logger.info(f"‚úÖ URL processed - Image: {image.size}, HTML: {len(html)} chars")
            
        else:
            logger.warning("‚ùå No input provided")
            return HTMLResponse(
                '<div class="error-message">‚ö†Ô∏è Please provide either a URL or upload an image file.</div>',
                status_code=400
            )
        
        # Run AI analysis
        logger.info(f"ü§ñ Starting AI analysis...")
        analysis = await analyze_with_openai(image, html)
        
        # Create annotated visualization with VIBRANT heatmap
        logger.info(f"üé® Creating visualization...")
        annotated_image, metrics = create_annotated_image(image, analysis)
        
        # Save results
        run_id = str(uuid.uuid4())[:8]
        image_path = Config.OUTPUTS_DIR / f"{run_id}.png"
        json_path = Config.OUTPUTS_DIR / f"{run_id}.json"
        
        annotated_image.convert('RGB').save(image_path, 'PNG', quality=95, optimize=True)
        
        with open(json_path, 'w') as f:
            json.dump(analysis, f, indent=2)
        
        logger.info(f"üíæ Results saved - Score: {metrics.get('overall_score', 0):.1f}/100")
        
        # Template context
        context = {
            "request": request,
            "run_id": run_id,
            "analysis": analysis,
            "metrics": metrics,
            "image_url": f"/outputs/{run_id}.png",
            "success": True
        }
        
        return templates.TemplateResponse("results.html", context)
        
    except Exception as e:
        logger.error(f"‚ùå Analysis failed for request {request_id}: {e}")
        error_msg = str(e) if Config.DEBUG else "Analysis failed. Please try again."
        return HTMLResponse(
            f'<div class="error-message">‚ùå {error_msg}</div>',
            status_code=500
        )

@app.get("/debug")
async def debug_info():
    """Enhanced debug endpoint"""
    import sys
    
    # Test OpenAI client creation
    openai_test_result = "Not tested"
    if OPENAI_AVAILABLE and Config.OPENAI_API_KEY:
        try:
            test_client = OpenAI(api_key=Config.OPENAI_API_KEY)
            openai_test_result = "‚úÖ Success"
        except Exception as e:
            openai_test_result = f"‚ùå Failed: {str(e)}"
    
    return {
        "status": "running",
        "python_version": sys.version,
        "openai_available": OPENAI_AVAILABLE,
        "playwright_available": PLAYWRIGHT_AVAILABLE,
        "openai_key_set": bool(Config.OPENAI_API_KEY),
        "openai_key_length": len(Config.OPENAI_API_KEY) if Config.OPENAI_API_KEY else 0,
        "openai_client_test": openai_test_result,
        "model": Config.OPENAI_MODEL,
        "directories": {
            "outputs": str(Config.OUTPUTS_DIR),
            "static": str(Config.STATIC_DIR),  
            "templates": str(Config.TEMPLATES_DIR)
        },
        "directories_exist": {
            "outputs": Config.OUTPUTS_DIR.exists(),
            "static": Config.STATIC_DIR.exists(),
            "templates": Config.TEMPLATES_DIR.exists()
        },
        "config": {
            "debug": Config.DEBUG,
            "host": Config.HOST,
            "port": Config.PORT,
            "max_image_size": Config.MAX_IMAGE_SIZE,
            "temperature": Config.TEMPERATURE
        }
    }

@app.get("/health")
async def health():
    """Health check"""
    return {
        "status": "üü¢ healthy",
        "openai_available": OPENAI_AVAILABLE and bool(Config.OPENAI_API_KEY),
        "playwright_available": PLAYWRIGHT_AVAILABLE,
        "version": "2.0.0-heatmap-fixed"
    }

@app.get("/download/{run_id}")
async def download_results(run_id: str, format: str = "png"):
    """Download results"""
    if format == "png":
        file_path = Config.OUTPUTS_DIR / f"{run_id}.png"
        if file_path.exists():
            return FileResponse(file_path, filename=f"attention-analysis-{run_id}.png")
    elif format == "json":
        file_path = Config.OUTPUTS_DIR / f"{run_id}.json"
        if file_path.exists():
            return FileResponse(file_path, filename=f"attention-analysis-{run_id}.json")
    
    raise HTTPException(404, f"File not found for format: {format}")

# ============================================================================
# RUN APPLICATION
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "="*80)
    print("üéØ ATTENTION OPTIMIZATION AI - HEATMAP FIXED VERSION")
    print("="*80)
    print(f"‚úÖ OpenAI Available: {'Yes' if OPENAI_AVAILABLE and Config.OPENAI_API_KEY else 'No'}")
    print(f"üîë API Key: {'Configured (' + str(len(Config.OPENAI_API_KEY)) + ' chars)' if Config.OPENAI_API_KEY else 'Missing'}")
    print(f"üì∑ Screenshots: {'Enabled' if PLAYWRIGHT_AVAILABLE else 'Disabled'}")
    print(f"ü§ñ AI Model: {Config.OPENAI_MODEL}")
    print(f"üåê Server: http://{Config.HOST}:{Config.PORT}")
    print("="*80)
    print("üé® HEATMAP FIXES:")
    print("  ‚úÖ Vibrant, highly visible heatmap colors")
    print("  ‚úÖ Enhanced alpha transparency (80-180)")
    print("  ‚úÖ 12x16 saliency grid validation")
    print("  ‚úÖ Fallback grid generation")
    print("  ‚úÖ Thick, visible CTA borders (5px)")
    print("  ‚úÖ Enhanced CTA labels with saliency %")
    print("  ‚úÖ Better color contrast and visibility")
    print("="*80)
    print("üöÄ OTHER FIXES:")
    print("  ‚úÖ OpenAI client initialization (no proxy args)")
    print("  ‚úÖ Form data processing (proper FastAPI types)")
    print("  ‚úÖ File upload handling with validation")
    print("  ‚úÖ URL processing with error handling")
    print("  ‚úÖ Enhanced debugging and logging")
    print("="*80)
    
    # Test OpenAI client initialization
    if Config.OPENAI_API_KEY and OPENAI_AVAILABLE:
        try:
            test_client = OpenAI(api_key=Config.OPENAI_API_KEY)
            print("‚úÖ OpenAI client test: SUCCESS")
        except Exception as e:
            print(f"‚ùå OpenAI client test: FAILED - {e}")
    elif not Config.OPENAI_API_KEY:
        print("‚ö†Ô∏è  Set OPENAI_API_KEY environment variable to enable analysis")
        print("   Example: export OPENAI_API_KEY='sk-your-key-here'")
    
    print("="*80)
    
    uvicorn.run(
        "main:app",
        host=Config.HOST,
        port=Config.PORT,
        reload=Config.DEBUG
    )